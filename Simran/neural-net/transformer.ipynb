{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6386643f-6c75-4b07-8f1d-1e9acfffd318",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from huggingface_hub import hf_hub_download\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "!set HF_HUB_DISABLE_SYMLINKS_WARNING=true\n",
    "\n",
    "# Load dataset\n",
    "dataset = 'reg_cat/analcatdata_supreme.csv'\n",
    "REPO_ID = \"inria-soda/tabular-benchmark\"\n",
    "data = pd.read_csv(\n",
    "    hf_hub_download(repo_id=REPO_ID, filename=dataset, repo_type=\"dataset\")\n",
    ")\n",
    "\n",
    "X = data.drop('Log_exposure', axis=1)\n",
    "y = data['Log_exposure']\n",
    "\n",
    "# Standardize continuous features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b09a94-8147-4b29-8d18-068e94adddbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-03 17:21:54,604] A new study created in memory with name: no-name-0dd996e0-5a23-44dd-bcad-a2869122b652\n",
      "[I 2024-06-03 17:22:21,042] Trial 0 finished with value: 0.5153371691703796 and parameters: {'num_heads': 2, 'embed_dim': 16, 'num_layers': 5, 'dropout': 0.22177243325976959, 'learning_rate': 0.00035895319981955594}. Best is trial 0 with value: 0.5153371691703796.\n",
      "[I 2024-06-03 17:23:42,168] Trial 1 finished with value: 0.7503073811531067 and parameters: {'num_heads': 5, 'embed_dim': 120, 'num_layers': 6, 'dropout': 0.2280790091688426, 'learning_rate': 1.8484811506031174e-05}. Best is trial 0 with value: 0.5153371691703796.\n",
      "[I 2024-06-03 17:25:06,970] Trial 2 finished with value: 0.9310290217399597 and parameters: {'num_heads': 7, 'embed_dim': 168, 'num_layers': 5, 'dropout': 0.49503873138427207, 'learning_rate': 0.0006581859085801193}. Best is trial 0 with value: 0.5153371691703796.\n",
      "[I 2024-06-03 17:25:42,156] Trial 3 finished with value: 0.7204582691192627 and parameters: {'num_heads': 7, 'embed_dim': 168, 'num_layers': 2, 'dropout': 0.28486167574784726, 'learning_rate': 5.148016571988966e-05}. Best is trial 0 with value: 0.5153371691703796.\n",
      "[I 2024-06-03 17:26:47,709] Trial 4 finished with value: 0.3959067165851593 and parameters: {'num_heads': 8, 'embed_dim': 128, 'num_layers': 5, 'dropout': 0.4478684670468802, 'learning_rate': 1.8027244985127046e-05}. Best is trial 4 with value: 0.3959067165851593.\n",
      "[I 2024-06-03 17:27:48,028] Trial 5 finished with value: 0.7565051913261414 and parameters: {'num_heads': 8, 'embed_dim': 256, 'num_layers': 3, 'dropout': 0.44167954549879185, 'learning_rate': 0.0005692827504479866}. Best is trial 4 with value: 0.3959067165851593.\n",
      "[I 2024-06-03 17:28:36,665] Trial 6 finished with value: 0.7891452312469482 and parameters: {'num_heads': 7, 'embed_dim': 112, 'num_layers': 4, 'dropout': 0.10541624599794602, 'learning_rate': 0.00015717176291725482}. Best is trial 4 with value: 0.3959067165851593.\n",
      "[I 2024-06-03 17:28:58,258] Trial 7 finished with value: 0.6348032355308533 and parameters: {'num_heads': 4, 'embed_dim': 96, 'num_layers': 2, 'dropout': 0.4822113955351435, 'learning_rate': 0.00041375166091808986}. Best is trial 4 with value: 0.3959067165851593.\n",
      "[I 2024-06-03 17:30:09,372] Trial 8 finished with value: 0.5170122385025024 and parameters: {'num_heads': 8, 'embed_dim': 256, 'num_layers': 4, 'dropout': 0.13972393017740936, 'learning_rate': 0.0012968879193556166}. Best is trial 4 with value: 0.3959067165851593.\n",
      "[I 2024-06-03 17:30:47,858] Trial 9 finished with value: 0.5182844400405884 and parameters: {'num_heads': 6, 'embed_dim': 144, 'num_layers': 3, 'dropout': 0.33272122558757067, 'learning_rate': 0.00010452968186701428}. Best is trial 4 with value: 0.3959067165851593.\n",
      "[I 2024-06-03 17:31:41,265] Trial 10 finished with value: 0.5604764223098755 and parameters: {'num_heads': 3, 'embed_dim': 48, 'num_layers': 6, 'dropout': 0.3975220013124478, 'learning_rate': 0.008998450800084077}. Best is trial 4 with value: 0.3959067165851593.\n",
      "[I 2024-06-03 17:32:17,898] Trial 11 finished with value: 0.5289489030838013 and parameters: {'num_heads': 2, 'embed_dim': 16, 'num_layers': 5, 'dropout': 0.2022904676593337, 'learning_rate': 1.1627023209167646e-05}. Best is trial 4 with value: 0.3959067165851593.\n",
      "[I 2024-06-03 17:32:54,922] Trial 12 finished with value: 0.518878161907196 and parameters: {'num_heads': 2, 'embed_dim': 16, 'num_layers': 5, 'dropout': 0.34938657196332207, 'learning_rate': 0.0019632547742830394}. Best is trial 4 with value: 0.3959067165851593.\n",
      "[I 2024-06-03 17:33:45,724] Trial 13 finished with value: 0.26530590653419495 and parameters: {'num_heads': 4, 'embed_dim': 64, 'num_layers': 5, 'dropout': 0.2565767626966763, 'learning_rate': 4.5915253774484096e-05}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:34:44,490] Trial 14 finished with value: 0.3710852265357971 and parameters: {'num_heads': 4, 'embed_dim': 64, 'num_layers': 6, 'dropout': 0.27365074119232724, 'learning_rate': 3.6317175325459633e-05}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:35:50,880] Trial 15 finished with value: 0.5325278639793396 and parameters: {'num_heads': 4, 'embed_dim': 64, 'num_layers': 6, 'dropout': 0.2754230076545729, 'learning_rate': 6.492360550295386e-05}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:37:02,849] Trial 16 finished with value: 0.5790428519248962 and parameters: {'num_heads': 4, 'embed_dim': 64, 'num_layers': 6, 'dropout': 0.1639430175799907, 'learning_rate': 3.133236458355347e-05}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:38:06,095] Trial 17 finished with value: 0.7374998927116394 and parameters: {'num_heads': 5, 'embed_dim': 80, 'num_layers': 6, 'dropout': 0.2503618090176996, 'learning_rate': 0.000138570645079959}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:38:40,144] Trial 18 finished with value: 0.5637304186820984 and parameters: {'num_heads': 3, 'embed_dim': 48, 'num_layers': 4, 'dropout': 0.3313458701545935, 'learning_rate': 3.8863010409734364e-05}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:39:18,501] Trial 19 finished with value: 0.41384005546569824 and parameters: {'num_heads': 5, 'embed_dim': 80, 'num_layers': 4, 'dropout': 0.3762869600480219, 'learning_rate': 8.79664663279932e-05}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:39:56,777] Trial 20 finished with value: 0.46283096075057983 and parameters: {'num_heads': 3, 'embed_dim': 48, 'num_layers': 5, 'dropout': 0.19205448747506074, 'learning_rate': 0.00018391694289796926}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:41:04,110] Trial 21 finished with value: 0.5394341945648193 and parameters: {'num_heads': 6, 'embed_dim': 144, 'num_layers': 5, 'dropout': 0.29962496215659484, 'learning_rate': 1.0159509753932894e-05}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:41:51,075] Trial 22 finished with value: 0.3457663953304291 and parameters: {'num_heads': 4, 'embed_dim': 64, 'num_layers': 5, 'dropout': 0.41952624239943487, 'learning_rate': 2.21895987476251e-05}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:42:51,994] Trial 23 finished with value: 0.5835180878639221 and parameters: {'num_heads': 4, 'embed_dim': 64, 'num_layers': 6, 'dropout': 0.26084619338642234, 'learning_rate': 2.490888242825081e-05}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:43:47,366] Trial 24 finished with value: 0.42800113558769226 and parameters: {'num_heads': 3, 'embed_dim': 48, 'num_layers': 5, 'dropout': 0.3808188212540395, 'learning_rate': 5.226807117667248e-05}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:44:58,345] Trial 25 finished with value: 0.3457707464694977 and parameters: {'num_heads': 4, 'embed_dim': 64, 'num_layers': 6, 'dropout': 0.30902273346730286, 'learning_rate': 1.974899393457824e-05}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:45:33,734] Trial 26 finished with value: 0.39931052923202515 and parameters: {'num_heads': 5, 'embed_dim': 80, 'num_layers': 3, 'dropout': 0.40535960564467133, 'learning_rate': 1.8126592812190716e-05}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:46:32,467] Trial 27 finished with value: 0.5744938850402832 and parameters: {'num_heads': 6, 'embed_dim': 96, 'num_layers': 5, 'dropout': 0.4354224758384211, 'learning_rate': 1.3659212130456724e-05}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:47:29,247] Trial 28 finished with value: 0.45121175050735474 and parameters: {'num_heads': 3, 'embed_dim': 48, 'num_layers': 6, 'dropout': 0.31791344525246706, 'learning_rate': 2.2418921947719812e-05}. Best is trial 13 with value: 0.26530590653419495.\n",
      "[I 2024-06-03 17:48:23,389] Trial 29 finished with value: 0.8274841904640198 and parameters: {'num_heads': 4, 'embed_dim': 64, 'num_layers': 5, 'dropout': 0.36501356471330193, 'learning_rate': 0.00022858751837748665}. Best is trial 13 with value: 0.26530590653419495.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'num_heads': 4, 'embed_dim': 64, 'num_layers': 5, 'dropout': 0.2565767626966763, 'learning_rate': 4.5915253774484096e-05}\n",
      "Best RMSE:  0.26530590653419495\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class EnhancedLagLlama(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers, dropout):\n",
    "        super(EnhancedLagLlama, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.decoder = nn.Linear(embed_dim, input_dim)\n",
    "        self.fc = nn.Linear(input_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)  # Add a sequence dimension\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.squeeze(1)  # Remove the sequence dimension\n",
    "        x = self.decoder(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def pretrain(model, dataloader, optimizer, criterion, device, mask_prob=0.15):\n",
    "    model.train()\n",
    "    for X_batch, _ in dataloader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        mask = (torch.rand(X_batch.shape) < mask_prob).float().to(device)\n",
    "        masked_X_batch = X_batch * (1 - mask)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(masked_X_batch)\n",
    "        loss = criterion(outputs, X_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    num_heads = trial.suggest_int('num_heads', 2, 8)\n",
    "    embed_dim = trial.suggest_int('embed_dim', num_heads * 8, num_heads * 32, step=num_heads * 8)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 6)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "\n",
    "    # Initialize the model\n",
    "    input_dim = X_train.shape[1]\n",
    "    model = EnhancedLagLlama(input_dim, embed_dim, num_heads, num_layers, dropout)\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Pre-training with feature masking\n",
    "    pretrain(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "    # Fine-tuning loop\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor.to(device))\n",
    "        rmse = torch.sqrt(criterion(predictions, y_test_tensor.to(device))).item()\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# Run the optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "print(\"Best RMSE: \", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634068b-60c6-4fd2-b3d2-0798ba65ea39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
