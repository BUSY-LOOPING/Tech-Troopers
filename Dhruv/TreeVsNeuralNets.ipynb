{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79b79a28-5703-400b-beb3-dd8075b702bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from hyperopt import hp, tpe, fmin, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc96866-487e-45f5-8781-1fe1183cf3b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!set HF_HUB_DISABLE_SYMLINKS_WARNING=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57ca08b4-5c21-4ef0-a0ce-9e402e52d662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPO_ID = \"inria-soda/tabular-benchmark\"\n",
    "clf_cat = [\n",
    "    'albert.csv', \n",
    "   'compas-two-years.csv', \n",
    "   'covertype.csv', \n",
    "   'default-of-credit-card-clients.csv',\n",
    "   'electricity.csv',\n",
    "   'eye_movements.csv',\n",
    "   'road-safety.csv'\n",
    "]\n",
    "\n",
    "clf_num = [\n",
    "    'Bioresponse.csv',\n",
    "    'Diabetes130US.csv',\n",
    "    'Higgs.csv',\n",
    "    'MagicTelescope.csv',\n",
    "    'MiniBooNE.csv',\n",
    "    'bank-marketing.csv',\n",
    "    'california.csv',\n",
    "    'covertype.csv',\n",
    "    'credit.csv',\n",
    "    'default-of-credit-card-clients.csv',\n",
    "    'electricity.csv',\n",
    "    'eye_movements.csv',\n",
    "    'heloc.csv',\n",
    "    'house_16H.csv',\n",
    "    'jannis.csv',\n",
    "    'pol.csv'\n",
    "]\n",
    "\n",
    "reg_cat = [\n",
    "    'Airlines_DepDelay_1M.csv',\n",
    "    'Allstate_Claims_Severity.csv',\n",
    "    'Bike_Sharing_Demand.csv',\n",
    "    'Brazilian houses.csv',\n",
    "    'Mercedes_Benz_Greener_Manufacturing.csv',\n",
    "    'SGEMM_GPU_kernel_performance.csv',\n",
    "    'abalone.csv',\n",
    "    'analcatdata_supreme.csv',\n",
    "    'delays_zurich_transport.csv',\n",
    "    'diamonds.csv',\n",
    "    'house_sales.csv',\n",
    "    'medical_charges.csv',\n",
    "    'nyc-taxi-green-dec-2016.csv',\n",
    "    'particulate-matter-ukair-2017.csv',\n",
    "    'seattlecrime6.csv',\n",
    "    'topo_2_1.csv',\n",
    "    'visualizing_soil.csv'\n",
    "]\n",
    "\n",
    "reg_num = [\n",
    "    'Ailerons.csv',\n",
    "    'Bike_Sharing_Demand.csv',\n",
    "    'Brazilian houses.csv',\n",
    "    'MiamiHousing2016.csv',\n",
    "    'abalone.csv',\n",
    "    'cpu_act.csv',\n",
    "    'delays_zurich_transport.csv',\n",
    "    'diamonds.csv',\n",
    "    'elevators.csv',\n",
    "    'house_16H.csv',\n",
    "    'house_sales.csv',\n",
    "    'houses.csv',\n",
    "    'medical_charges.csv',\n",
    "    'nyc-taxi-green-dec-2016.csv',\n",
    "    'pol.csv',\n",
    "    'sulfur.csv',\n",
    "    'superconduct.csv',\n",
    "    'wine_quality.csv',\n",
    "    'yprop_4_1.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8bc8f90c-87e1-4db8-b7f5-227331fa7fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_pseudo_categorical(X, y):\n",
    "    \"\"\"Remove columns where most values are the same\"\"\"\n",
    "    pseudo_categorical_cols_mask = X.nunique() < 10\n",
    "    print(\"Removed {} columns with pseudo-categorical values on {} columns\".format(sum(pseudo_categorical_cols_mask),\n",
    "                                                                                   X.shape[1]))\n",
    "    X = X.drop(X.columns[pseudo_categorical_cols_mask], axis=1)\n",
    "    return X, y\n",
    "\n",
    "def remove_rows_with_missing_values(X, y):\n",
    "    missing_rows_mask = pd.isnull(X).any(axis=1)\n",
    "    print(\"Removed {} rows with missing values on {} rows\".format(sum(missing_rows_mask), X.shape[0]))\n",
    "    X = X[~missing_rows_mask]\n",
    "    y = y[~missing_rows_mask]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def remove_missing_values(X, y, threshold=0.7):\n",
    "    \"\"\"Remove columns where most values are missing, then remove any row with missing values\"\"\"\n",
    "    missing_cols_mask = pd.isnull(X).mean(axis=0) > threshold\n",
    "    print(\"Removed {} columns with missing values on {} columns\".format(sum(missing_cols_mask), X.shape[1]))\n",
    "    X = X.drop(X.columns[missing_cols_mask], axis=1)\n",
    "    missing_rows_mask = pd.isnull(X).any(axis=1)\n",
    "    print(\"Removed {} rows with missing values on {} rows\".format(sum(missing_rows_mask), X.shape[0]))\n",
    "    X = X[~missing_rows_mask]\n",
    "    y = y[~missing_rows_mask]\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "def remove_high_cardinality(X, y, categorical_mask, threshold=20):\n",
    "    high_cardinality_mask = (X.nunique() > threshold).values\n",
    "    print(\"high cardinality columns: {}\".format(X.columns[high_cardinality_mask * categorical_mask]))\n",
    "    n_high_cardinality = sum(categorical_mask * high_cardinality_mask)\n",
    "    X = X.drop(X.columns[categorical_mask * high_cardinality_mask], axis=1)\n",
    "    print(\"Removed {} high-cardinality categorical features\".format(n_high_cardinality))\n",
    "    categorical_mask = [categorical_mask[i] for i in range(len(categorical_mask)) if not (high_cardinality_mask[i] and categorical_mask[i])]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def balance(X, y) :\n",
    "    freq_count = y.value_counts().sort(ascending=False)\n",
    "    X = X[y in freq_count.index[:2]]\n",
    "    y = y[y in freq_count.index[:2]]\n",
    "    \n",
    "def transform_target(y, keyword):\n",
    "    if keyword == \"log\":\n",
    "        return np.sign(y) * np.log(1 + np.abs(y))\n",
    "    elif keyword == \"none\":\n",
    "        return y\n",
    "    elif pd.isnull(keyword):\n",
    "        return y\n",
    "    \n",
    "def is_heavy_tailed(data):\n",
    "    \"\"\"\n",
    "    Checks if the distribution of the given data is heavy-tailed.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): The data for which to check the tail behavior.\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if the distribution is heavy-tailed, False otherwise.\n",
    "    \"\"\"\n",
    "    skewness = skew(data)\n",
    "    kurt = kurtosis(data)\n",
    "    \n",
    "    # Thresholds for skewness and kurtosis to determine if distribution is heavy-tailed\n",
    "    skew_threshold = 0\n",
    "    kurtosis_threshold = 3\n",
    "    \n",
    "    return skewness > skew_threshold or kurt > kurtosis_threshold\n",
    "    \n",
    "    \n",
    "def preprocess_data(X, y, isCategorical=False):\n",
    "    # df = df.copy()\n",
    "#     for col in df.select_dtypes(include='object').columns:\n",
    "#         if df[col].nunique() > 20:\n",
    "#             df = df.drop(col, axis=1)\n",
    "    \n",
    "#     # Removing numerical features with less than 10 unique values\n",
    "#     for col in df.select_dtypes(include=['int', 'float']).columns:\n",
    "#         if df[col].nunique() < 10:\n",
    "#             df.drop(col, axis=1, inplace=True)\n",
    "#         elif df[col].nunique() == 2:  # Converting numerical features with 2 unique values to categorical\n",
    "#             df[col] = df[col].astype('category')\n",
    "\n",
    "    if isCategorical :\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    elif is_heavy_tailed(y) :\n",
    "        y = transform_target(y, keyword='log')\n",
    "\n",
    "\n",
    "    X, y = remove_rows_with_missing_values(X, y)\n",
    "    X, y = remove_missing_values(X, y)\n",
    "    X, y = remove_pseudo_categorical(X, y)\n",
    "    categorical_mask = [(X[col].dtype == 'object' or len(X[col].unique()) < 20) for col in X.columns]\n",
    "    X, y = remove_high_cardinality(X, y, categorical_mask)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def split_data(X, y, isCategorical=None):\n",
    "    # Shuffle the data\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    try :\n",
    "        X = X.values\n",
    "    except :\n",
    "        pass\n",
    "    \n",
    "    try :\n",
    "        y = y.values\n",
    "    except :\n",
    "        pass\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    stratify = None if not isCategorical else y\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size=0.7, random_state=42, stratify=stratify)\n",
    "    stratify = None if not isCategorical else y_temp\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.7, random_state=42, stratify=stratify)\n",
    "    \n",
    "    num_val = min(X_val.shape[0], 50000)\n",
    "    num_test = min(X_test.shape[0], 50000)\n",
    "    \n",
    "    X_val, y_val = X_val[:num_val], y_val[:num_val]\n",
    "    X_test, y_test = X_test[:num_val], y_test[:num_test]\n",
    "\n",
    "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n",
    "\n",
    "\n",
    "def plot_distribution(data):\n",
    "    \"\"\"\n",
    "    Plots the distribution of the given data.\n",
    "    \n",
    "    Parameters:\n",
    "    data (array-like): The data to be plotted.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(data, bins=30, color='blue', alpha=0.7)\n",
    "    plt.title('Distribution Plot')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "def affine_renormalization_classification(results):\n",
    "    \"\"\"\n",
    "    Perform affine renormalization on classification results.\n",
    "    \n",
    "    Parameters:\n",
    "    results (list): List of original classification results between 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of renormalized classification results between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Find the top-performing model's accuracy\n",
    "    top_accuracy = max(results)\n",
    "    \n",
    "    # Find the accuracy corresponding to the 10th percentile\n",
    "    quantile_accuracy = np.percentile(results, 10)\n",
    "    \n",
    "    # Calculate the range of accuracies for renormalization\n",
    "    range_accuracy = top_accuracy - quantile_accuracy\n",
    "    \n",
    "    # Perform affine renormalization for each accuracy\n",
    "    renormalized_results = [(accuracy - quantile_accuracy) / range_accuracy for accuracy in results]\n",
    "    \n",
    "    return renormalized_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ecabdd-ef0d-4a75-81d5-0de3688f158c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>Distance</th>\n",
       "      <th>DepDelay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>1535.0</td>\n",
       "      <td>2556.0</td>\n",
       "      <td>2.197225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2035.0</td>\n",
       "      <td>2110.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1446.0</td>\n",
       "      <td>2475.0</td>\n",
       "      <td>1.945910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>1512.0</td>\n",
       "      <td>2586.0</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>930.0</td>\n",
       "      <td>1149.0</td>\n",
       "      <td>2399.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>835.0</td>\n",
       "      <td>940.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>-0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>1225.0</td>\n",
       "      <td>1633.0</td>\n",
       "      <td>843.0</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>1515.0</td>\n",
       "      <td>1735.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>-1.098612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>1335.0</td>\n",
       "      <td>1646.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>2.197225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>530.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>723.0</td>\n",
       "      <td>-2.079442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Month  DayofMonth  CRSDepTime  CRSArrTime  Distance  DepDelay\n",
       "0          10          11      1300.0      1535.0    2556.0  2.197225\n",
       "1          10          10      2035.0      2110.0     100.0 -1.386294\n",
       "2          10          26      1200.0      1446.0    2475.0  1.945910\n",
       "3          10           9      1145.0      1512.0    2586.0  0.693147\n",
       "4          10          16       930.0      1149.0    2399.0  0.000000\n",
       "...       ...         ...         ...         ...       ...       ...\n",
       "999995      7          30       835.0       940.0     317.0 -0.693147\n",
       "999996      7          29      1225.0      1633.0     843.0  0.693147\n",
       "999997      7          30      1515.0      1735.0     350.0 -1.098612\n",
       "999998      7          25      1335.0      1646.0     900.0  2.197225\n",
       "999999      7          31       530.0       645.0     723.0 -2.079442\n",
       "\n",
       "[1000000 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    hf_hub_download(repo_id=REPO_ID, filename=f'reg_cat/{reg_cat[0]}', repo_type=\"dataset\")\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74207333-6e5d-4da7-a02b-658ef4bd6154",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating albert.csv...\n",
      "Removed 0 rows with missing values on 58252 rows\n",
      "Removed 0 columns with missing values on 31 columns\n",
      "Removed 0 rows with missing values on 58252 rows\n",
      "Removed 7 columns with pseudo-categorical values on 31 columns\n",
      "high cardinality columns: Index([], dtype='object')\n",
      "Removed 0 high-cardinality categorical features\n",
      " 16%|███████▏                                    | 65/400 [14:55<1:16:56, 13.78s/trial, best loss: -0.6501335368180083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_cat_random_forest_scores = []\n",
    "clf_cat_gradient_boosting_scores = []\n",
    "for dataset in clf_cat :\n",
    "    print(f'Evaluating {dataset}...')\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        hf_hub_download(repo_id=REPO_ID, filename=f'clf_cat/{dataset}', repo_type=\"dataset\")\n",
    "    )\n",
    "    X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "    X, y = preprocess_data(X, y, isCategorical=True)\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = split_data(X, y, isCategorical=True)\n",
    "    for idx, iter_ in enumerate(range(15)) :\n",
    "        \n",
    "        #Random Forest\n",
    "        hyperparameter_space = {\n",
    "            'criterion': hp.choice('criterion', ['gini', 'entropy']),\n",
    "            'n_estimators': scope.int(hp.loguniform('n_estimators', np.log(9.5), np.log(3000.5))),\n",
    "            'max_depth': hp.choice('max_depth', [None, 2, 3, 4]),\n",
    "            'min_samples_split': hp.choice('min_samples_split', [2, 3]),\n",
    "            'min_samples_leaf': scope.int(hp.loguniform('min_samples_leaf', np.log(1.5), np.log(50.5))),\n",
    "            'bootstrap': hp.choice('bootstrap', [True, False]),\n",
    "            'max_features': hp.choice('max_features', ['sqrt', 'sqrt', 'log2', None, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "            'min_impurity_decrease': hp.choice('min_impurity_decrease', [0.0, 0.01, 0.02, 0.05])\n",
    "        }\n",
    "\n",
    "        def objective_function(hyperparameters):\n",
    "            global iteration_n\n",
    "            if iteration_n == 0:\n",
    "                model = RandomForestClassifier()\n",
    "            else :\n",
    "                model = RandomForestClassifier(**hyperparameters)\n",
    "            iteration_n += 1\n",
    "            # score = -np.mean(cross_val_score(model, train_data.iloc[:, :-1], train_data.iloc[:, -1], cv=5, scoring='accuracy'))\n",
    "            model.fit(X_train, y_train)\n",
    "            y_predict = model.predict(X_val)\n",
    "            score = -accuracy_score(y_val, y_predict)\n",
    "\n",
    "            return score\n",
    "\n",
    "        trials = Trials()\n",
    "        iteration_n = 0\n",
    "        best_hyperparameters = fmin(objective_function, hyperparameter_space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "        scores_round = -1 * np.array(trials.losses())\n",
    "        print(f'Best accuracy : {scores_round.max()}')\n",
    "        \n",
    "        clf_cat_random_forest_scores.append([scores_round.tolist()])\n",
    "        \n",
    "    \n",
    "np.save('clf_cat_random_forest_scores.npy', clf_cat_random_forest_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53ea9fd0-7ed5-4005-aa7b-7c347cdbd4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.652422739412438, 0.6327737504769172]]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_cat_random_forest_scores = []\n",
    "# reg_cat_gradient_boosting_scores = []\n",
    "for dataset in reg_cat :\n",
    "    print(f'Evaluating {dataset}...')\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        hf_hub_download(repo_id=REPO_ID, filename=f'reg_cat/{dataset}', repo_type=\"dataset\")\n",
    "    )\n",
    "    X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "    X, y = preprocess_data(X, y, isCategorical=True)\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = split_data(X, y, isCategorical=True)\n",
    "    for idx, iter_ in enumerate(range(15)) :\n",
    "        hyperparameter_space = {\n",
    "            'criterion', hp.choice('criterion', ['squared_error', 'absolute_error']),\n",
    "            'n_estimators': scope.int(hp.loguniform('n_estimators', np.log(9.5), np.log(3000.5))),\n",
    "            'max_depth': hp.choice('max_depth', [None, 2, 3, 4]),\n",
    "            'min_samples_split': hp.choice('min_samples_split', [2, 3]),\n",
    "            'min_samples_leaf': scope.int(hp.loguniform('min_samples_leaf', np.log(1.5), np.log(50.5))),\n",
    "            'bootstrap': hp.choice('bootstrap', [True, False]),\n",
    "            'max_features': hp.choice('max_features', ['sqrt', 'sqrt', 'log2', None, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "            'min_impurity_decrease': hp.choice('min_impurity_decrease', [0.0, 0.01, 0.02, 0.05])\n",
    "        }\n",
    "\n",
    "        def objective_function(hyperparameters):\n",
    "            global iteration_n\n",
    "            if iteration_n == 0:\n",
    "                model = RandomForestRegressor()\n",
    "            else :\n",
    "                model = RandomForestRegressor(**hyperparameters)\n",
    "            iteration_n += 1\n",
    "            # score = -np.mean(cross_val_score(model, train_data.iloc[:, :-1], train_data.iloc[:, -1], cv=5, scoring='accuracy'))\n",
    "            model.fit(X_train, y_train)\n",
    "            y_predict = model.predict(X_val)\n",
    "            score = -r2_score(y_val, y_predict)\n",
    "\n",
    "            return score\n",
    "\n",
    "        trials = Trials()\n",
    "        iteration_n = 0\n",
    "        best_hyperparameters = fmin(objective_function, hyperparameter_space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "        scores_round = -1 * np.array(trials.losses())\n",
    "        print(f'Best accuracy : {scores_round.max()}')\n",
    "        \n",
    "        reg_cat_random_forest_scores.append([scores_round.tolist()])\n",
    "        \n",
    "np.save('reg_cat_random_forest_scores.npy', reg_cat_random_forest_scores)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa66688-414b-4054-a27f-621a4338fe70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
