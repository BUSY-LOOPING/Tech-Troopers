{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79b79a28-5703-400b-beb3-dd8075b702bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from hyperopt import hp, tpe, fmin, Trials\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from preprocessing.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc96866-487e-45f5-8781-1fe1183cf3b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!set HF_HUB_DISABLE_SYMLINKS_WARNING=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57ca08b4-5c21-4ef0-a0ce-9e402e52d662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REPO_ID = \"inria-soda/tabular-benchmark\"\n",
    "clf_cat = [\n",
    "    'albert.csv', \n",
    "   'compas-two-years.csv', \n",
    "   # 'covertype.csv', \n",
    "   # 'default-of-credit-card-clients.csv',\n",
    "   # 'electricity.csv',\n",
    "   # 'eye_movements.csv',\n",
    "   # 'road-safety.csv'\n",
    "]\n",
    "\n",
    "clf_num = [\n",
    "    'Bioresponse.csv',\n",
    "    'Diabetes130US.csv',\n",
    "    'Higgs.csv',\n",
    "    'MagicTelescope.csv',\n",
    "    'MiniBooNE.csv',\n",
    "    'bank-marketing.csv',\n",
    "    'california.csv',\n",
    "    'covertype.csv',\n",
    "    'credit.csv',\n",
    "    'default-of-credit-card-clients.csv',\n",
    "    'electricity.csv',\n",
    "    'eye_movements.csv',\n",
    "    'heloc.csv',\n",
    "    'house_16H.csv',\n",
    "    'jannis.csv',\n",
    "    'pol.csv'\n",
    "]\n",
    "\n",
    "reg_cat = [\n",
    "    'Airlines_DepDelay_1M.csv',\n",
    "    'Allstate_Claims_Severity.csv',\n",
    "    'Bike_Sharing_Demand.csv',\n",
    "    'Brazilian houses.csv',\n",
    "    'Mercedes_Benz_Greener_Manufacturing.csv',\n",
    "    'SGEMM_GPU_kernel_performance.csv',\n",
    "    'abalone.csv',\n",
    "    'analcatdata_supreme.csv',\n",
    "    'delays_zurich_transport.csv',\n",
    "    'diamonds.csv',\n",
    "    'house_sales.csv',\n",
    "    'medical_charges.csv',\n",
    "    'nyc-taxi-green-dec-2016.csv',\n",
    "    'particulate-matter-ukair-2017.csv',\n",
    "    'seattlecrime6.csv',\n",
    "    'topo_2_1.csv',\n",
    "    'visualizing_soil.csv'\n",
    "]\n",
    "\n",
    "reg_num = [\n",
    "    'Ailerons.csv',\n",
    "    'Bike_Sharing_Demand.csv',\n",
    "    'Brazilian houses.csv',\n",
    "    'MiamiHousing2016.csv',\n",
    "    'abalone.csv',\n",
    "    'cpu_act.csv',\n",
    "    'delays_zurich_transport.csv',\n",
    "    'diamonds.csv',\n",
    "    'elevators.csv',\n",
    "    'house_16H.csv',\n",
    "    'house_sales.csv',\n",
    "    'houses.csv',\n",
    "    'medical_charges.csv',\n",
    "    'nyc-taxi-green-dec-2016.csv',\n",
    "    'pol.csv',\n",
    "    'sulfur.csv',\n",
    "    'superconduct.csv',\n",
    "    'wine_quality.csv',\n",
    "    'yprop_4_1.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f70905f7-8b2e-4df0-8af2-61795bc0cc8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_hpt_from_space(space, n_samples = 100) :\n",
    "    return [sample(space) for _ in range(n_samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b301a47-c547-4dae-9a23-941920c66248",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: {'bootstrap': False, 'criterion': 'gini', 'max_depth': 4, 'max_features': 0.6, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 15, 'min_samples_split': 2, 'n_estimators': 22}\n",
      "Sample 2: {'bootstrap': True, 'criterion': 'entropy', 'max_depth': 4, 'max_features': 0.6, 'min_impurity_decrease': 0.02, 'min_samples_leaf': 26, 'min_samples_split': 2, 'n_estimators': 254}\n",
      "Sample 3: {'bootstrap': True, 'criterion': 'gini', 'max_depth': 2, 'max_features': 0.3, 'min_impurity_decrease': 0.01, 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 10}\n",
      "Sample 4: {'bootstrap': True, 'criterion': 'entropy', 'max_depth': 4, 'max_features': 0.2, 'min_impurity_decrease': 0.05, 'min_samples_leaf': 48, 'min_samples_split': 2, 'n_estimators': 32}\n",
      "Sample 5: {'bootstrap': False, 'criterion': 'gini', 'max_depth': 4, 'max_features': 'sqrt', 'min_impurity_decrease': 0.0, 'min_samples_leaf': 22, 'min_samples_split': 3, 'n_estimators': 1615}\n",
      "Sample 6: {'bootstrap': True, 'criterion': 'gini', 'max_depth': 2, 'max_features': 0.9, 'min_impurity_decrease': 0.02, 'min_samples_leaf': 47, 'min_samples_split': 3, 'n_estimators': 270}\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_space = {\n",
    "    'criterion': hp.choice('criterion', ['gini', 'entropy']),\n",
    "    'n_estimators': scope.int(hp.loguniform('n_estimators', np.log(9.5), np.log(3000.5))),\n",
    "    'max_depth': hp.choice('max_depth', [None, 2, 3, 4]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 3]),\n",
    "    'min_samples_leaf': scope.int(hp.loguniform('min_samples_leaf', np.log(1.5), np.log(50.5))),\n",
    "    'bootstrap': hp.choice('bootstrap', [True, False]),\n",
    "    'max_features': hp.choice('max_features', ['sqrt', 'sqrt', 'log2', None, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "    'min_impurity_decrease': hp.choice('min_impurity_decrease', [0.0, 0.01, 0.02, 0.05])\n",
    "}\n",
    "\n",
    "# Generate 100 samples from the hyperparameter space\n",
    "samples = [sample(hyperparameter_space) for _ in range(100)]\n",
    "\n",
    "# Display the samples\n",
    "for i, s in enumerate(samples):\n",
    "    if i > 5 :\n",
    "        break\n",
    "    print(f\"Sample {i+1}: {s}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05ecabdd-ef0d-4a75-81d5-0de3688f158c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 rows with missing values on 58252 rows\n",
      "Removed 0 columns with missing values on 31 columns\n",
      "Removed 0 rows with missing values on 58252 rows\n",
      "Removed 7 columns with pseudo-categorical values on 31 columns\n",
      "high cardinality columns: Index([], dtype='object')\n",
      "Removed 0 high-cardinality categorical features\n",
      "100%|█████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.94s/trial, best loss: -0.6316291491797024]\n",
      "Best hyperparameters: {'bootstrap': False, 'criterion': 'gini', 'max_depth': 4, 'max_features': 0.1, 'min_impurity_decrease': 0.02, 'min_samples_leaf': 30, 'min_samples_split': 2, 'n_estimators': 86}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    hf_hub_download(repo_id=REPO_ID, filename=f'clf_cat/{clf_cat[0]}', repo_type=\"dataset\")\n",
    ")\n",
    "\n",
    "# Split and preprocess the data\n",
    "X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "X, y = preprocess_data(X, y, isCategorical=True)\n",
    "(X_train, y_train), (X_val, y_val), (X_test, y_test) = split_data(X, y, isCategorical=True)\n",
    "\n",
    "random_forest_hyperparameter_space = {\n",
    "    'criterion': hp.choice('criterion', ['gini', 'entropy']),\n",
    "    'n_estimators': scope.int(hp.loguniform('n_estimators', np.log(9.5), np.log(3000.5))),\n",
    "    'max_depth': hp.choice('max_depth', [None, 2, 3, 4]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 3]),\n",
    "    'min_samples_leaf': scope.int(hp.loguniform('min_samples_leaf', np.log(1.5), np.log(50.5))),\n",
    "    'bootstrap': hp.choice('bootstrap', [True, False]),\n",
    "    'max_features': hp.choice('max_features', ['sqrt', 'sqrt', 'log2', None, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "    'min_impurity_decrease': hp.choice('min_impurity_decrease', [0.0, 0.01, 0.02, 0.05])\n",
    "}\n",
    "\n",
    "n_samples = 2\n",
    "random_forest_hpt_samples = sample_hpt_from_space(random_forest_hyperparameter_space, n_samples)\n",
    "\n",
    "# Define the objective function\n",
    "def objective_function(trial_index):\n",
    "    hyperparameters = random_forest_hpt_samples[trial_index]\n",
    "    model = RandomForestClassifier(**hyperparameters)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predict = model.predict(X_val)\n",
    "    score = -accuracy_score(y_val, y_predict)\n",
    "    return score\n",
    "\n",
    "# Wrap the objective function to pass the trial index\n",
    "def wrapped_objective_function(trial):\n",
    "    trial_index = trial['index']\n",
    "    return objective_function(trial_index)\n",
    "\n",
    "# Define the search space for the trial index\n",
    "trial_index_space = hp.choice('index', range(n_samples))\n",
    "\n",
    "# Use hyperopt to optimize the objective function\n",
    "trials = Trials()\n",
    "best_trial = fmin(wrapped_objective_function, {'index': trial_index_space}, algo=tpe.suggest, max_evals=n_samples, trials=trials)\n",
    "\n",
    "# Extract the best trial index\n",
    "best_trial_index = best_trial['index']\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_hyperparameters = random_forest_hpt_samples[best_trial_index]\n",
    "print(\"Best hyperparameters:\", best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "566580fb-d756-4ceb-a66e-d14ca7abd275",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6316291491797024"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.best_trial['result']['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f74fb097-cf30-48a9-9943-33dff9e19da9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Loss = -0.6316291491797024, Hyperparameters = {'bootstrap': False, 'criterion': 'gini', 'max_depth': 4, 'max_features': 0.1, 'min_impurity_decrease': 0.02, 'min_samples_leaf': 30, 'min_samples_split': 2, 'n_estimators': 86}\n",
      "Iteration 2: Loss = -0.6316291491797024, Hyperparameters = {'bootstrap': False, 'criterion': 'gini', 'max_depth': 4, 'max_features': 0.1, 'min_impurity_decrease': 0.02, 'min_samples_leaf': 30, 'min_samples_split': 2, 'n_estimators': 86}\n"
     ]
    }
   ],
   "source": [
    "for i, trial in enumerate(trials.trials):\n",
    "    print(f\"Iteration {i+1}: Loss = {trial['result']['loss']}, Hyperparameters = {random_forest_hpt_samples[trial['misc']['vals']['index'][0]]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "504892d7-a95d-4f85-abaf-0dd757868361",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[28,  8, 53, ..., 12, 83, 92],\n",
       "       [16, 67, 99, ..., 34, 88, 64],\n",
       "       [40, 61, 10, ..., 52, 15, 56],\n",
       "       ...,\n",
       "       [86,  6, 39, ..., 59, 76, 45],\n",
       "       [85, 64,  8, ..., 26, 79, 15],\n",
       "       [68, 75, 72, ..., 51, 18, 27]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([np.random.permutation(100) for _ in range(15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74207333-6e5d-4da7-a02b-658ef4bd6154",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating albert.csv...\n",
      "Removed 0 rows with missing values on 58252 rows\n",
      "Removed 0 columns with missing values on 31 columns\n",
      "Removed 0 rows with missing values on 58252 rows\n",
      "Removed 7 columns with pseudo-categorical values on 31 columns\n",
      "high cardinality columns: Index([], dtype='object')\n",
      "Removed 0 high-cardinality categorical features\n",
      "Training data : 40776\n",
      "Validation data : 5242\n",
      "Testing data : 5242\n",
      "Running Random Forest...\n",
      "  0%|                                                      | 0/2 [00:00<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('gradient-boosting-results') :\n",
    "    os.makedirs('gradient-boosting-results')\n",
    "\n",
    "if not os.path.exists('random-forest-results') :\n",
    "    os.makedirs('random-forest-results')\n",
    "    \n",
    "if not os.path.exists('xgboost-results') :\n",
    "    os.makedirs('xgboost-results')\n",
    "\n",
    "np.random.seed(42)\n",
    "n_iterations = 2\n",
    "n_shuffles = 15\n",
    "random_shuffle_indices = np.array([np.random.permutation(n_iterations) for _ in range(n_shuffles)])\n",
    "\n",
    "clf_cat_random_forest_scores = np.zeros((len(clf_cat), n_shuffles))\n",
    "clf_cat_gradient_boosting_scores = np.zeros((len(clf_cat), n_shuffles))\n",
    "clf_cat_xgboost_scores = np.zeros((len(clf_cat), n_shuffles))\n",
    "\n",
    "random_forest_hyperparameter_space = {\n",
    "    'criterion': hp.choice('criterion', ['gini', 'entropy']),\n",
    "    'n_estimators': scope.int(hp.loguniform('n_estimators', np.log(9.5), np.log(3000.5))),\n",
    "    'max_depth': hp.choice('max_depth', [None, 2, 3, 4]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 3]),\n",
    "    'min_samples_leaf': scope.int(hp.loguniform('min_samples_leaf', np.log(1.5), np.log(50.5))),\n",
    "    'bootstrap': hp.choice('bootstrap', [True, False]),\n",
    "    'max_features': hp.choice('max_features', ['sqrt', 'sqrt', 'log2', None, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "    'min_impurity_decrease': hp.choice('min_impurity_decrease', [0.0, 0.01, 0.02, 0.05])\n",
    "}\n",
    "random_forest_hpt_samples = sample_hpt_from_space(random_forest_hyperparameter_space, n_samples=n_iterations)\n",
    "\n",
    "gradient_boosting_hyperparameter_space = {\n",
    "    'loss': hp.choice('loss', ['exponential', 'log_loss']),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(10)),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'n_estimators': scope.int(hp.qloguniform('n_estimators', np.log(10.5), np.log(1000.5), 1)),\n",
    "    'criterion': hp.choice('criterion', ['friedman_mse', 'squared_error']),\n",
    "    'max_depth': hp.pchoice('max_depth', [(0.1, None), (0.1, 2), (0.1, 3), (0.6, 4), (0.1, 5)]),\n",
    "    'min_samples_split': hp.pchoice('min_samples_split', [(0.95, 2), (0.05, 3)]),\n",
    "    'min_samples_leaf': scope.int(hp.qloguniform('min_samples_leaf', np.log(1.5), np.log(50.5), 1)),\n",
    "    'min_impurity_decrease': hp.pchoice('min_impurity_decrease', [(0.85, 0.0), (0.05, 0.01), (0.05, 0.02), (0.05, 0.05)]),\n",
    "    'max_leaf_nodes': hp.pchoice('max_leaf_nodes', [(0.85, None), (0.05, 5), (0.05, 10), (0.05, 15)])\n",
    "}\n",
    "gradient_boosting_hpt_samples = sample_hpt_from_space(gradient_boosting_hyperparameter_space, n_samples=n_iterations)\n",
    "\n",
    "xgboost_hyperparameter_space = {\n",
    "    'max_depth': hp.uniformint('max_depth', 1, 11),\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 6000, 200),\n",
    "    'min_child_weight': hp.qloguniform('min_child_weight', np.log(1), np.log(1e2), 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(1e-5), np.log(0.7)),\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "    'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(7)),\n",
    "    'lambda': hp.loguniform('lambda', np.log(1), np.log(4)),\n",
    "    'alpha': hp.loguniform('alpha', np.log(1e-8), np.log(1e2))\n",
    "}\n",
    "xgboost_hpt_samples = sample_hpt_from_space(xgboost_hyperparameter_space, n_samples=n_iterations)\n",
    "\n",
    "\n",
    "for dataset_idx, dataset in enumerate(clf_cat) :\n",
    "    for random_shuffle_idx in range(n_shuffles) :\n",
    "        print(f'Evaluating {dataset}...')\n",
    "\n",
    "        df = pd.read_csv(\n",
    "            hf_hub_download(repo_id=REPO_ID, filename=f'clf_cat/{dataset}', repo_type=\"dataset\")\n",
    "        )\n",
    "        X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "        X, y = preprocess_data(X, y, isCategorical=True)\n",
    "        (X_train, y_train), (X_val, y_val), (X_test, y_test) = split_data(X, y, isCategorical=True)\n",
    "        print(f'Training data : {len(X_train)}\\nValidation data : {len(X_val)}\\nTesting data : {len(X_test)}')\n",
    "\n",
    "        #Random Forest\n",
    "        print('Running Random Forest...')\n",
    "\n",
    "        # Define the objective function\n",
    "        def objective_function(trial_index):\n",
    "            hyperparameters = random_forest_hpt_samples[trial_index]\n",
    "            model = RandomForestClassifier(**hyperparameters)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_predict = model.predict(X_val)\n",
    "            score = -accuracy_score(y_val, y_predict)\n",
    "            return score\n",
    "        \n",
    "        def wrapped_objective_function(trial):\n",
    "            trial_index = trial['index']\n",
    "            trial_index = random_shuffle_indices[random_shuffle_idx][trial_index]\n",
    "            return objective_function(trial_index)\n",
    "\n",
    "        # Define the search space for the trial index\n",
    "        trial_index_space = hp.choice('index', range(n_samples))\n",
    "\n",
    "        # Use hyperopt to optimize the objective function\n",
    "        trials = Trials()\n",
    "        best_trial = fmin(wrapped_objective_function, {'index': trial_index_space}, algo=tpe.suggest, max_evals=n_iterations, trials=trials)\n",
    "\n",
    "        # Extract the best trial index\n",
    "        best_trial_index = best_trial['index']\n",
    "\n",
    "        # Retrieve the best hyperparameters\n",
    "        best_hyperparameters = random_forest_hpt_samples[best_trial_index]\n",
    "        with open(os.path.join('random-forest-results', f'{dataset}_shuffle_{random_shuffle_idx}.pkl'), 'wb') as f :\n",
    "            pickle.dump(trials.trials, f)\n",
    "                  \n",
    "        clf_cat_random_forest_scores[dataset_idx, random_shuffle_idx] = trials.best_trial['result']['loss'] * -1\n",
    "\n",
    "        # Gradient Boosting\n",
    "        print('Running Gradient Boosting')\n",
    "\n",
    "        def objective_function(trial_index):\n",
    "            hyperparameters = gradient_boosting_hpt_samples[trial_index]\n",
    "            model = GradientBoostingClassifier(**hyperparameters)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_predict = model.predict(X_val)\n",
    "            score = -accuracy_score(y_val, y_predict)\n",
    "            return score\n",
    "\n",
    "        trials = Trials()\n",
    "        best_trial = fmin(wrapped_objective_function, {'index': trial_index_space}, algo=tpe.suggest, max_evals=n_iterations, trials=trials)\n",
    "\n",
    "        # Extract the best trial index\n",
    "        best_trial_index = best_trial['index']\n",
    "\n",
    "        # Retrieve the best hyperparameters\n",
    "        best_hyperparameters = gradient_boosting_hpt_samples[best_trial_index]\n",
    "        with open(os.path.join('gradient-boosting-results', f'{dataset}_shuffle_{random_shuffle_idx}.pkl'), 'wb') as f :\n",
    "            pickle.dump(trials.trials, f)\n",
    "                  \n",
    "        clf_cat_gradient_boosting_scores[dataset_idx, random_shuffle_idx] = trials.best_trial['result']['loss'] * -1\n",
    "\n",
    "\n",
    "        # XGBoost\n",
    "        print('Running XGBoosting')\n",
    "\n",
    "        def objective_function(trial_index):\n",
    "            hyperparameters = gradient_boosting_hpt_samples[trial_index]\n",
    "            model = XGBClassifier(**hyperparameters)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_predict = model.predict(X_val)\n",
    "            score = -accuracy_score(y_val, y_predict)\n",
    "            return score\n",
    "\n",
    "        trials = Trials()\n",
    "        best_trial = fmin(wrapped_objective_function, {'index': trial_index_space}, algo=tpe.suggest, max_evals=n_iterations, trials=trials)\n",
    "\n",
    "        # Extract the best trial index\n",
    "        best_trial_index = best_trial['index']\n",
    "\n",
    "        # Retrieve the best hyperparameters\n",
    "        best_hyperparameters = gradient_boosting_hpt_samples[best_trial_index]\n",
    "        with open(os.path.join('xgboost-results', f'{dataset}_shuffle_{random_shuffle_idx}.pkl'), 'wb') as f :\n",
    "            pickle.dump(trials.trials, f)\n",
    "                  \n",
    "        clf_cat_xgboost_scores[dataset_idx, random_shuffle_idx] = trials.best_trial['result']['loss'] * -1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa66688-414b-4054-a27f-621a4338fe70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "data_science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
